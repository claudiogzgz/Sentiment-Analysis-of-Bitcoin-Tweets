{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f506447",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "#pip freeze > requirements.txt"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9ae4e050",
   "metadata": {},
   "source": [
    "# Task 2\n",
    "\n",
    "**Claudio Gonzalez Gonzalez**\n",
    "\n",
    "Environment Setup:\n",
    "- Required python version: 3.12.2\n",
    "- requirements.txt available in the folder"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5964bae4",
   "metadata": {},
   "source": [
    "## **Cell 0-1: Load and preprocess data**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "26e56a0b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# - Classes depend on data loading and preprocessing.\n",
    "# - Preprocessor should be built first and used across all models.\n",
    "import pandas as pd\n",
    "import re\n",
    "\n",
    "class DataHandler:\n",
    "    \"\"\"\n",
    "    Handles loading and preprocessing of tweet datasets.\n",
    "    \"\"\"\n",
    "    def __init__(self, train_path, test_path):\n",
    "        self.train_path = train_path\n",
    "        self.test_path = test_path\n",
    "        self.df_train = None\n",
    "        self.df_test = None\n",
    "\n",
    "    def load_data(self):\n",
    "        \"\"\"Load training and test datasets (.csv or .parquet).\"\"\"\n",
    "        if self.train_path.endswith(\".parquet.gzip\"):\n",
    "            self.df_train = pd.read_parquet(self.train_path)\n",
    "            self.df_test = pd.read_parquet(self.test_path)\n",
    "        else:\n",
    "            self.df_train = pd.read_csv(self.train_path)\n",
    "            self.df_test = pd.read_csv(self.test_path)\n",
    "\n",
    "        # Ensure consistent indexing\n",
    "        self.df_train.reset_index(drop=True, inplace=True)\n",
    "        self.df_test.reset_index(drop=True, inplace=True)\n",
    "        print(f\"Train shape: {self.df_train.shape}, Test shape: {self.df_test.shape}\")\n",
    "        return self.df_train, self.df_test\n",
    "\n",
    "\n",
    "class TweetPreprocessor:\n",
    "    \"\"\"\n",
    "    Preprocess tweets by removing noise and normalizing text.\n",
    "    \"\"\"\n",
    "    def __init__(self):\n",
    "        self.patterns = {\n",
    "            \"html\": re.compile(r\"<[^>]+>\"),\n",
    "            \"urls\": re.compile(r\"http\\S+|www\\S+|https\\S+\"),\n",
    "            \"unicode\": re.compile(r\"\\\\u[\\dA-Fa-f]{4}\"),\n",
    "            \"non_ascii\": re.compile(r\"[^\\x00-\\x7F]+\"),\n",
    "            \"punct\": re.compile(r\"[^a-z\\s]\")\n",
    "        }\n",
    "\n",
    "    def clean(self, text: str) -> str:\n",
    "        \"\"\"Clean a single tweet.\"\"\"\n",
    "        text = text.lower()\n",
    "        text = self.patterns[\"html\"].sub(\"\", text)\n",
    "        text = self.patterns[\"urls\"].sub(\"\", text)\n",
    "        text = self.patterns[\"unicode\"].sub(\"\", text)\n",
    "        text = self.patterns[\"non_ascii\"].sub(\"\", text)\n",
    "        text = self.patterns[\"punct\"].sub(\"\", text)\n",
    "        text = \" \".join(text.split())\n",
    "        return text\n",
    "\n",
    "    def apply(self, df):\n",
    "        \"\"\"Apply cleaning to a DataFrame with 'content' column.\"\"\"\n",
    "        df[\"clean_text\"] = df[\"content\"].apply(self.clean)\n",
    "        return df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8b176128",
   "metadata": {},
   "source": [
    "## **Cell 2: Dictionary-based sentiment analysis (VADER)**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "8edaaeae",
   "metadata": {},
   "outputs": [],
   "source": [
    "# - Initializes the VADER analyzer\n",
    "# - Predicts sentiment for a DataFrame\n",
    "# - Stores predictions for evaluation\n",
    "\n",
    "import nltk\n",
    "from nltk.sentiment.vader import SentimentIntensityAnalyzer\n",
    "\n",
    "class DictionarySentimentAnalyzer:\n",
    "    \"\"\"\n",
    "    Applies a lexicon-based sentiment analysis (VADER).\n",
    "    \"\"\"\n",
    "    def __init__(self):\n",
    "        nltk.download(\"vader_lexicon\")\n",
    "        self.analyzer = SentimentIntensityAnalyzer()\n",
    "\n",
    "    def predict(self, df):\n",
    "        \"\"\"\n",
    "        Apply sentiment analysis to a DataFrame.\n",
    "        Returns the DataFrame with a new column 'dict_pred'.\n",
    "        \"\"\"\n",
    "        def vader_sentiment(text):\n",
    "            score = self.analyzer.polarity_scores(text)[\"compound\"]\n",
    "            return int(score >= 0)\n",
    "\n",
    "        df[\"dict_pred\"] = df[\"clean_text\"].apply(vader_sentiment)\n",
    "        return df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ad525aab",
   "metadata": {},
   "source": [
    "## **Cell 3: TF-IDF + Logistic Regression classifier**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "936e229e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# - Initializes TF-IDF and Logistic Regression\n",
    "# - Trains on the cleaned training data\n",
    "# - Predicts on test data\n",
    "# - Optionally returns predictions for evaluation\n",
    "\n",
    "\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "class TfidfLogisticClassifier:\n",
    "    \"\"\"\n",
    "    TF-IDF vectorizer + Logistic Regression sentiment classifier.\n",
    "    \"\"\"\n",
    "    def __init__(self, max_features=5000, max_iter=200):\n",
    "        self.vectorizer = TfidfVectorizer(max_features=max_features)\n",
    "        self.model = LogisticRegression(max_iter=max_iter)\n",
    "\n",
    "    def train(self, df_train):\n",
    "        \"\"\"\n",
    "        Fit TF-IDF and train Logistic Regression.\n",
    "        \"\"\"\n",
    "        X_train = self.vectorizer.fit_transform(df_train[\"clean_text\"])\n",
    "        y_train = df_train[\"sentiment\"].astype(int)\n",
    "        self.model.fit(X_train, y_train)\n",
    "\n",
    "    def predict(self, df_test):\n",
    "        \"\"\"\n",
    "        Transform test data and predict sentiment.\n",
    "        Adds 'tfidf_pred' column to the DataFrame.\n",
    "        \"\"\"\n",
    "        X_test = self.vectorizer.transform(df_test[\"clean_text\"])\n",
    "        preds = self.model.predict(X_test)\n",
    "        df_test[\"tfidf_pred\"] = preds\n",
    "        return df_test"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "08255ce8",
   "metadata": {},
   "source": [
    "## **Cell 4: RNN Classifier with Own Embeddings**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "3b9d819e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# - Tokenize text and pad sequences\n",
    "# - Build an LSTM-based RNN model with learned embeddings\n",
    "# - Handle train/validation split and early stopping\n",
    "# - Train, predict, and optionally evaluate performance\n",
    "\n",
    "\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Embedding, LSTM, Dense\n",
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "\n",
    "class RNNClassifier:\n",
    "    \"\"\"\n",
    "    RNN-based sentiment classifier with custom embeddings.\n",
    "    \"\"\"\n",
    "    def __init__(self, vocab_size=10000, embedding_dim=64, lstm_units=64, max_len=50):\n",
    "        self.vocab_size = vocab_size\n",
    "        self.embedding_dim = embedding_dim\n",
    "        self.lstm_units = lstm_units\n",
    "        self.max_len = max_len\n",
    "        self.tokenizer = Tokenizer(num_words=vocab_size, oov_token=\"<OOV>\")\n",
    "        self.model = None\n",
    "\n",
    "    def preprocess(self, texts):\n",
    "        \"\"\"\n",
    "        Tokenize and pad sequences for input to RNN.\n",
    "        \"\"\"\n",
    "        seqs = self.tokenizer.texts_to_sequences(texts)\n",
    "        return pad_sequences(seqs, maxlen=self.max_len, padding='post', truncating='post')\n",
    "\n",
    "    def build_model(self):\n",
    "        \"\"\"\n",
    "        Build the RNN model with embedding + LSTM.\n",
    "        \"\"\"\n",
    "        model = Sequential([\n",
    "            Embedding(input_dim=self.vocab_size, output_dim=self.embedding_dim, input_length=self.max_len),\n",
    "            LSTM(self.lstm_units),\n",
    "            Dense(1, activation=\"sigmoid\")\n",
    "        ])\n",
    "        model.compile(loss=\"binary_crossentropy\", optimizer=\"adam\", metrics=[\"accuracy\"])\n",
    "        self.model = model\n",
    "\n",
    "    def train(self, df_train, val_size=0.2, epochs=10, batch_size=32):\n",
    "        \"\"\"\n",
    "        Fit tokenizer, prepare data, train the model with early stopping.\n",
    "        \"\"\"\n",
    "        texts = df_train[\"clean_text\"]\n",
    "        labels = df_train[\"sentiment\"].astype(int)\n",
    "        \n",
    "        # Fit tokenizer and transform data\n",
    "        self.tokenizer.fit_on_texts(texts)\n",
    "        X = self.preprocess(texts)\n",
    "        y = labels\n",
    "\n",
    "        # Split into training/validation sets\n",
    "        X_train, X_val, y_train, y_val = train_test_split(X, y, test_size=val_size, random_state=42)\n",
    "\n",
    "        # Build and train model\n",
    "        self.build_model()\n",
    "        early_stop = EarlyStopping(monitor=\"val_loss\", patience=2, restore_best_weights=True)\n",
    "        self.model.fit(X_train, y_train, validation_data=(X_val, y_val),\n",
    "                       epochs=epochs, batch_size=batch_size, callbacks=[early_stop], verbose=1)\n",
    "\n",
    "    def predict(self, df_test):\n",
    "        \"\"\"\n",
    "        Tokenize and predict sentiment for the test set.\n",
    "        \"\"\"\n",
    "        X_test = self.preprocess(df_test[\"clean_text\"])\n",
    "        preds = (self.model.predict(X_test) > 0.5).astype(int)\n",
    "        df_test[\"rnn_pred\"] = preds\n",
    "        return df_test"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "54fefc0c",
   "metadata": {},
   "source": [
    "## **Cell 5: RNN Classifier with pretrained GloVe embeddings**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "b0e521c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# - Tokenize and pad text sequences (shared logic)\n",
    "# - Load pretrained GloVe embeddings\n",
    "# - Build an RNN with the embedding layer initialized from GloVe\n",
    "# - Train with validation split and early stopping\n",
    "# - Predict sentiments on the test set\n",
    "\n",
    "import os\n",
    "import zipfile\n",
    "import urllib.request\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Embedding, LSTM, Dense, Dropout\n",
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "\n",
    "class RNNGloveClassifier:\n",
    "    \"\"\"\n",
    "    RNN-based sentiment classifier using pretrained GloVe embeddings.\n",
    "    \"\"\"\n",
    "    def __init__(self, vocab_size=10000, embedding_dim=100, lstm_units=64, max_len=50):\n",
    "        self.vocab_size = vocab_size\n",
    "        self.embedding_dim = embedding_dim\n",
    "        self.lstm_units = lstm_units\n",
    "        self.max_len = max_len\n",
    "        self.tokenizer = Tokenizer(num_words=vocab_size, oov_token=\"<OOV>\")\n",
    "        self.model = None\n",
    "        self.embedding_matrix = None\n",
    "\n",
    "    def preprocess(self, texts):\n",
    "        \"\"\"\n",
    "        Tokenize and pad sequences for RNN input.\n",
    "        \"\"\"\n",
    "        seqs = self.tokenizer.texts_to_sequences(texts)\n",
    "        return pad_sequences(seqs, maxlen=self.max_len, padding='post', truncating='post')\n",
    "\n",
    "    def load_glove_embeddings(self, glove_path=\"glove.6B.100d.txt\"):\n",
    "        \"\"\"\n",
    "        Download (if needed) and load GloVe embeddings.\n",
    "        \"\"\"\n",
    "        if not os.path.exists(glove_path):\n",
    "            print(\"Downloading GloVe embeddings...\")\n",
    "            url = \"http://nlp.stanford.edu/data/glove.6B.zip\"\n",
    "            urllib.request.urlretrieve(url, \"glove.6B.zip\")\n",
    "            with zipfile.ZipFile(\"glove.6B.zip\", \"r\") as zip_ref:\n",
    "                zip_ref.extract(\"glove.6B.100d.txt\")\n",
    "        \n",
    "        embeddings_index = {}\n",
    "        with open(glove_path, encoding='utf8') as f:\n",
    "            for line in f:\n",
    "                values = line.split()\n",
    "                word = values[0]\n",
    "                coeffs = np.asarray(values[1:], dtype='float32')\n",
    "                embeddings_index[word] = coeffs\n",
    "        \n",
    "        word_index = self.tokenizer.word_index\n",
    "        embedding_matrix = np.zeros((self.vocab_size, self.embedding_dim))\n",
    "        for word, i in word_index.items():\n",
    "            if i < self.vocab_size:\n",
    "                vector = embeddings_index.get(word)\n",
    "                if vector is not None:\n",
    "                    embedding_matrix[i] = vector\n",
    "        self.embedding_matrix = embedding_matrix\n",
    "\n",
    "    def build_model(self):\n",
    "        \"\"\"\n",
    "        Build the RNN model with pretrained embeddings.\n",
    "        \"\"\"\n",
    "        model = Sequential([\n",
    "            Embedding(input_dim=self.vocab_size, output_dim=self.embedding_dim, \n",
    "                      weights=[self.embedding_matrix], input_length=self.max_len, trainable=False),\n",
    "            LSTM(self.lstm_units),\n",
    "            Dropout(0.3),\n",
    "            Dense(1, activation=\"sigmoid\")\n",
    "        ])\n",
    "        model.compile(loss=\"binary_crossentropy\", optimizer=\"adam\", metrics=[\"accuracy\"])\n",
    "        self.model = model\n",
    "\n",
    "    def train(self, df_train, val_size=0.2, epochs=10, batch_size=32):\n",
    "        \"\"\"\n",
    "        Fit tokenizer, build embedding matrix, and train model.\n",
    "        \"\"\"\n",
    "        texts = df_train[\"clean_text\"]\n",
    "        labels = df_train[\"sentiment\"].astype(int)\n",
    "\n",
    "        # Fit tokenizer\n",
    "        self.tokenizer.fit_on_texts(texts)\n",
    "        X = self.preprocess(texts)\n",
    "        y = labels\n",
    "\n",
    "        # Load GloVe embeddings\n",
    "        self.load_glove_embeddings()\n",
    "\n",
    "        # Split train/validation\n",
    "        X_train, X_val, y_train, y_val = train_test_split(X, y, test_size=val_size, random_state=42)\n",
    "\n",
    "        # Build and train model\n",
    "        self.build_model()\n",
    "        early_stop = EarlyStopping(monitor=\"val_loss\", patience=2, restore_best_weights=True)\n",
    "        self.model.fit(X_train, y_train, validation_data=(X_val, y_val),\n",
    "                       epochs=epochs, batch_size=batch_size, callbacks=[early_stop], verbose=1)\n",
    "\n",
    "    def predict(self, df_test):\n",
    "        \"\"\"\n",
    "        Predict sentiments on test set.\n",
    "        \"\"\"\n",
    "        X_test = self.preprocess(df_test[\"clean_text\"])\n",
    "        preds = (self.model.predict(X_test) > 0.5).astype(int)\n",
    "        df_test[\"rnn_glove_pred\"] = preds\n",
    "        return df_test"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6db68086",
   "metadata": {},
   "source": [
    "## **Cell 6: Huggingface Transformer Pipeline (pre-trained sentiment model)**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "c2a3a025",
   "metadata": {},
   "outputs": [],
   "source": [
    "# - Load a pre-trained Huggingface model for sentiment analysis (e.g., DistilBERT)\n",
    "# - Use the pipeline() API to generate predictions\n",
    "# - Map results to binary sentiment labels\n",
    "# - Attach predictions to the test DataFrame\n",
    "\n",
    "\n",
    "from transformers import pipeline\n",
    "\n",
    "class TransformerPipelineSentiment:\n",
    "    \"\"\"\n",
    "    Uses a Huggingface pre-trained transformer for sentiment analysis\n",
    "    without additional fine-tuning.\n",
    "    \"\"\"\n",
    "    def __init__(self, model_name=\"distilbert-base-uncased-finetuned-sst-2-english\"):\n",
    "        self.model_name = model_name\n",
    "        self.classifier = pipeline(\"sentiment-analysis\", model=self.model_name)\n",
    "\n",
    "    def predict(self, df_test, batch_size=32):\n",
    "        \"\"\"\n",
    "        Apply the transformer pipeline to the test DataFrame.\n",
    "        Adds 'transformer_pred' column with binary predictions.\n",
    "        \"\"\"\n",
    "        texts = df_test[\"clean_text\"].tolist()\n",
    "        results = self.classifier(texts, truncation=True, padding=True, batch_size=batch_size)\n",
    "        preds = [1 if r[\"label\"].upper() == \"POSITIVE\" else 0 for r in results]\n",
    "        df_test[\"transformer_pred\"] = preds\n",
    "        return df_test"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e94023c8",
   "metadata": {},
   "source": [
    "## **Cell 7: Fine-tune DistilBERT Transformer (Uses Huggingface Trainer for inheritance-based fine-tuning)**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "83871d08",
   "metadata": {},
   "outputs": [],
   "source": [
    "# - Fine-tune a DistilBERT transformer for binary sentiment classification.\n",
    "# - Tokenize train and test datasets for Huggingface Trainer API.\n",
    "# - Train for 1 epoch with evaluation metrics (Accuracy, Precision, Recall, F1).\n",
    "# - Store a trained Trainer object for predictions.\n",
    "# - Add a finetuned_pred column to the test DataFrame for results aggregation.\n",
    "\n",
    "\n",
    "\n",
    "from transformers import DistilBertTokenizerFast, DistilBertForSequenceClassification, Trainer, TrainingArguments\n",
    "from datasets import Dataset\n",
    "import numpy as np\n",
    "from sklearn.metrics import accuracy_score, precision_recall_fscore_support\n",
    "\n",
    "class DistilBERTFineTuner(TransformerPipelineSentiment):\n",
    "    \"\"\"\n",
    "    Inherits from TransformerPipelineSentiment.\n",
    "    Adds fine-tuning capability using Huggingface Trainer API.\n",
    "    \"\"\"\n",
    "    def __init__(self, model_name=\"distilbert-base-uncased\", num_labels=2):\n",
    "        super().__init__(model_name)\n",
    "        self.tokenizer = DistilBertTokenizerFast.from_pretrained(model_name)\n",
    "        self.model = DistilBertForSequenceClassification.from_pretrained(model_name, num_labels=num_labels)\n",
    "        self.trainer = None\n",
    "\n",
    "    def _compute_metrics(self, pred):\n",
    "        labels = pred.label_ids\n",
    "        preds = np.argmax(pred.predictions, axis=1)\n",
    "        precision, recall, f1, _ = precision_recall_fscore_support(labels, preds, average='binary')\n",
    "        acc = accuracy_score(labels, preds)\n",
    "        return {\"accuracy\": acc, \"precision\": precision, \"recall\": recall, \"f1\": f1}\n",
    "\n",
    "    def train(self, df_train, df_test, epochs=1):\n",
    "        \"\"\"\n",
    "        Fine-tune the transformer on training data.\n",
    "        \"\"\"\n",
    "        train_encodings = self.tokenizer(df_train[\"clean_text\"].tolist(), truncation=True, padding=True)\n",
    "        test_encodings = self.tokenizer(df_test[\"clean_text\"].tolist(), truncation=True, padding=True)\n",
    "\n",
    "        train_dataset = Dataset.from_dict({\n",
    "            \"input_ids\": train_encodings[\"input_ids\"],\n",
    "            \"attention_mask\": train_encodings[\"attention_mask\"],\n",
    "            \"labels\": df_train[\"sentiment\"].astype(int).tolist()\n",
    "        })\n",
    "\n",
    "        test_dataset = Dataset.from_dict({\n",
    "            \"input_ids\": test_encodings[\"input_ids\"],\n",
    "            \"attention_mask\": test_encodings[\"attention_mask\"],\n",
    "            \"labels\": df_test[\"sentiment\"].astype(int).tolist()\n",
    "        })\n",
    "\n",
    "        training_args = TrainingArguments(\n",
    "            output_dir=\"./results\",\n",
    "            num_train_epochs=epochs,\n",
    "            per_device_train_batch_size=16,\n",
    "            per_device_eval_batch_size=64,\n",
    "            save_strategy=\"no\",\n",
    "            logging_dir=\"./logs\",\n",
    "            logging_steps=50,\n",
    "            seed=42\n",
    "        )\n",
    "\n",
    "        self.trainer = Trainer(\n",
    "            model=self.model,\n",
    "            args=training_args,\n",
    "            train_dataset=train_dataset,\n",
    "            eval_dataset=test_dataset,\n",
    "            compute_metrics=self._compute_metrics\n",
    "        )\n",
    "\n",
    "        self.trainer.train()\n",
    "\n",
    "    def predict(self, df_test):\n",
    "        \"\"\"\n",
    "        Use the fine-tuned model for predictions.\n",
    "        \"\"\"\n",
    "        test_encodings = self.tokenizer(df_test[\"clean_text\"].tolist(), truncation=True, padding=True)\n",
    "        test_dataset = Dataset.from_dict({\n",
    "            \"input_ids\": test_encodings[\"input_ids\"],\n",
    "            \"attention_mask\": test_encodings[\"attention_mask\"],\n",
    "            \"labels\": df_test[\"sentiment\"].astype(int).tolist()\n",
    "        })\n",
    "\n",
    "        preds = self.trainer.predict(test_dataset).predictions\n",
    "        preds = np.argmax(preds, axis=1)\n",
    "        df_test[\"finetuned_pred\"] = preds\n",
    "        return df_test"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b5395292",
   "metadata": {},
   "source": [
    "## **Cell 8: Evaluate all models**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "8a8e57e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# - Accept predictions from multiple models\n",
    "# - Compute binary classification metrics (Accuracy, Precision, Recall, F1)\n",
    "# - Store results for each model\n",
    "# - Generate a summary DataFrame (for Cell 9)\n",
    "\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n",
    "import pandas as pd\n",
    "\n",
    "class EvaluationManager:\n",
    "    \"\"\"\n",
    "    Collects predictions from multiple models and computes evaluation metrics.\n",
    "    \"\"\"\n",
    "    def __init__(self):\n",
    "        self.results = {}\n",
    "\n",
    "    def add_results(self, model_name, y_true, y_pred):\n",
    "        \"\"\"\n",
    "        Adds evaluation metrics for a given model.\n",
    "        \"\"\"\n",
    "        metrics = {\n",
    "            \"accuracy\": accuracy_score(y_true, y_pred),\n",
    "            \"precision\": precision_score(y_true, y_pred),\n",
    "            \"recall\": recall_score(y_true, y_pred),\n",
    "            \"f1\": f1_score(y_true, y_pred)\n",
    "        }\n",
    "        self.results[model_name] = metrics\n",
    "\n",
    "    def get_summary(self, sort_by=\"f1\"):\n",
    "        \"\"\"\n",
    "        Returns a sorted DataFrame of all model results.\n",
    "        \"\"\"\n",
    "        df_results = pd.DataFrame(self.results).T\n",
    "        df_results = df_results.round(4)\n",
    "        return df_results.sort_values(sort_by, ascending=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b4a21d3b",
   "metadata": {},
   "source": [
    "## **Main Cell: Execute Entire Sentiment Analysis Pipeline**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "583dddbd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train shape: (1500, 5), Test shape: (500, 5)\n",
      "Epoch 1/10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package vader_lexicon to\n",
      "[nltk_data]     /Users/claudio/nltk_data...\n",
      "[nltk_data]   Package vader_lexicon is already up-to-date!\n",
      "/opt/anaconda3/lib/python3.12/site-packages/keras/src/layers/core/embedding.py:97: UserWarning: Argument `input_length` is deprecated. Just remove it.\n",
      "  warnings.warn(\n",
      "E0000 00:00:1754084618.746520 9028013 meta_optimizer.cc:967] PluggableGraphOptimizer failed: INVALID_ARGUMENT: Failed to deserialize the `graph_buf`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m38/38\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 23ms/step - accuracy: 0.7956 - loss: 0.5589 - val_accuracy: 0.8367 - val_loss: 0.4500\n",
      "Epoch 2/10\n",
      "\u001b[1m38/38\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 18ms/step - accuracy: 0.7923 - loss: 0.5005 - val_accuracy: 0.8367 - val_loss: 0.4484\n",
      "Epoch 3/10\n",
      "\u001b[1m38/38\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 17ms/step - accuracy: 0.8054 - loss: 0.4877 - val_accuracy: 0.8367 - val_loss: 0.4321\n",
      "Epoch 4/10\n",
      "\u001b[1m38/38\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 17ms/step - accuracy: 0.8554 - loss: 0.3965 - val_accuracy: 0.8367 - val_loss: 0.4806\n",
      "Epoch 5/10\n",
      "\u001b[1m38/38\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 16ms/step - accuracy: 0.9043 - loss: 0.3147 - val_accuracy: 0.8400 - val_loss: 0.5444\n",
      "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step\n",
      "Epoch 1/10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/lib/python3.12/site-packages/keras/src/layers/core/embedding.py:97: UserWarning: Argument `input_length` is deprecated. Just remove it.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m38/38\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 21ms/step - accuracy: 0.7777 - loss: 0.6319 - val_accuracy: 0.8267 - val_loss: 0.4609\n",
      "Epoch 2/10\n",
      "\u001b[1m38/38\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 15ms/step - accuracy: 0.7920 - loss: 0.4943 - val_accuracy: 0.8200 - val_loss: 0.4519\n",
      "Epoch 3/10\n",
      "\u001b[1m38/38\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 15ms/step - accuracy: 0.8078 - loss: 0.4776 - val_accuracy: 0.8200 - val_loss: 0.4281\n",
      "Epoch 4/10\n",
      "\u001b[1m38/38\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 15ms/step - accuracy: 0.8161 - loss: 0.4249 - val_accuracy: 0.8233 - val_loss: 0.3981\n",
      "Epoch 5/10\n",
      "\u001b[1m38/38\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 15ms/step - accuracy: 0.8246 - loss: 0.3912 - val_accuracy: 0.8167 - val_loss: 0.3987\n",
      "Epoch 6/10\n",
      "\u001b[1m38/38\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 15ms/step - accuracy: 0.8385 - loss: 0.3968 - val_accuracy: 0.8100 - val_loss: 0.3982\n",
      "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Device set to use mps:0\n",
      "Some weights of DistilBertForSequenceClassification were not initialized from the model checkpoint at distilbert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight', 'pre_classifier.bias', 'pre_classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Device set to use mps:0\n",
      "Some weights of DistilBertForSequenceClassification were not initialized from the model checkpoint at distilbert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight', 'pre_classifier.bias', 'pre_classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "/opt/anaconda3/lib/python3.12/site-packages/torch/utils/data/dataloader.py:683: UserWarning: 'pin_memory' argument is set as true but not supported on MPS now, then device pinned memory won't be used.\n",
      "  warnings.warn(warn_msg)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='94' max='94' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [94/94 00:23, Epoch 1/1]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>50</td>\n",
       "      <td>0.470200</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/lib/python3.12/site-packages/torch/utils/data/dataloader.py:683: UserWarning: 'pin_memory' argument is set as true but not supported on MPS now, then device pinned memory won't be used.\n",
      "  warnings.warn(warn_msg)\n"
     ]
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Final Sentiment Model Performance Summary (ordered by F1 score):\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>accuracy</th>\n",
       "      <th>precision</th>\n",
       "      <th>recall</th>\n",
       "      <th>f1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>DistilBERT Fine-Tuned</th>\n",
       "      <td>0.866</td>\n",
       "      <td>0.8770</td>\n",
       "      <td>0.9703</td>\n",
       "      <td>0.9213</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>RNN GloVe</th>\n",
       "      <td>0.828</td>\n",
       "      <td>0.8472</td>\n",
       "      <td>0.9604</td>\n",
       "      <td>0.9002</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>RNN Custom Emb</th>\n",
       "      <td>0.818</td>\n",
       "      <td>0.8380</td>\n",
       "      <td>0.9604</td>\n",
       "      <td>0.8950</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>TF-IDF + Logistic</th>\n",
       "      <td>0.810</td>\n",
       "      <td>0.8109</td>\n",
       "      <td>0.9975</td>\n",
       "      <td>0.8946</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>VADER Lexicon</th>\n",
       "      <td>0.824</td>\n",
       "      <td>0.8744</td>\n",
       "      <td>0.9134</td>\n",
       "      <td>0.8935</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Huggingface Pipeline</th>\n",
       "      <td>0.452</td>\n",
       "      <td>0.9514</td>\n",
       "      <td>0.3391</td>\n",
       "      <td>0.5000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                       accuracy  precision  recall      f1\n",
       "DistilBERT Fine-Tuned     0.866     0.8770  0.9703  0.9213\n",
       "RNN GloVe                 0.828     0.8472  0.9604  0.9002\n",
       "RNN Custom Emb            0.818     0.8380  0.9604  0.8950\n",
       "TF-IDF + Logistic         0.810     0.8109  0.9975  0.8946\n",
       "VADER Lexicon             0.824     0.8744  0.9134  0.8935\n",
       "Huggingface Pipeline      0.452     0.9514  0.3391  0.5000"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# - Calls all classes and methods from previous cells  \n",
    "# - Loads data, preprocesses text  \n",
    "# - Runs all models (Dictionary, TF-IDF, RNNs, Transformers)  \n",
    "# - Fine-tunes **DistilBERT** using Huggingface Trainer  \n",
    "# - Evaluates all approaches and generates a summary table\n",
    "\n",
    "# -------------------------------\n",
    "# Cell 0 & 1: Load and preprocess data\n",
    "# -------------------------------\n",
    "data_handler = DataHandler(\"btc_tweets_train.parquet.gzip\", \"btc_tweets_test.parquet.gzip\")\n",
    "df_train, df_test = data_handler.load_data()\n",
    "\n",
    "preprocessor = TweetPreprocessor()\n",
    "df_train = preprocessor.apply(df_train)\n",
    "df_test = preprocessor.apply(df_test)\n",
    "\n",
    "# -------------------------------\n",
    "# Cell 2: Dictionary-based sentiment analysis (VADER)\n",
    "# -------------------------------\n",
    "dict_analyzer = DictionarySentimentAnalyzer()\n",
    "df_test = dict_analyzer.predict(df_test)\n",
    "\n",
    "# -------------------------------\n",
    "# Cell 3: TF-IDF + Logistic Regression classifier\n",
    "# -------------------------------\n",
    "tfidf_clf = TfidfLogisticClassifier()\n",
    "tfidf_clf.train(df_train)\n",
    "df_test = tfidf_clf.predict(df_test)\n",
    "\n",
    "# -------------------------------\n",
    "# Cell 4: RNN with custom embeddings\n",
    "# -------------------------------\n",
    "rnn_clf = RNNClassifier()\n",
    "rnn_clf.train(df_train)\n",
    "df_test = rnn_clf.predict(df_test)\n",
    "\n",
    "# -------------------------------\n",
    "# Cell 5: RNN with pretrained GloVe embeddings\n",
    "# -------------------------------\n",
    "rnn_glove_clf = RNNGloveClassifier()\n",
    "rnn_glove_clf.train(df_train)\n",
    "df_test = rnn_glove_clf.predict(df_test)\n",
    "\n",
    "# -------------------------------\n",
    "# Cell 6: Huggingface Transformer Pipeline (pre-trained sentiment model)\n",
    "# -------------------------------\n",
    "transformer_pipe = TransformerPipelineSentiment()\n",
    "df_test = transformer_pipe.predict(df_test)\n",
    "\n",
    "# -------------------------------\n",
    "# Cell 7: Fine-tune DistilBERT Transformer\n",
    "# -------------------------------\n",
    "distilbert_finetuner = DistilBERTFineTuner()\n",
    "distilbert_finetuner.train(df_train, df_test)\n",
    "df_test = distilbert_finetuner.predict(df_test)\n",
    "\n",
    "\n",
    "\n",
    "# -------------------------------\n",
    "# Cell 8: Evaluate all models\n",
    "# -------------------------------\n",
    "eval_mgr = EvaluationManager()\n",
    "eval_mgr.add_results(\"VADER Lexicon\", df_test[\"sentiment\"], df_test[\"dict_pred\"])\n",
    "eval_mgr.add_results(\"TF-IDF + Logistic\", df_test[\"sentiment\"], df_test[\"tfidf_pred\"])\n",
    "eval_mgr.add_results(\"RNN Custom Emb\", df_test[\"sentiment\"], df_test[\"rnn_pred\"])\n",
    "eval_mgr.add_results(\"RNN GloVe\", df_test[\"sentiment\"], df_test[\"rnn_glove_pred\"])\n",
    "eval_mgr.add_results(\"Huggingface Pipeline\", df_test[\"sentiment\"], df_test[\"transformer_pred\"])\n",
    "eval_mgr.add_results(\"DistilBERT Fine-Tuned\", df_test[\"sentiment\"], df_test[\"finetuned_pred\"])\n",
    "\n",
    "# -------------------------------\n",
    "# Final summary table\n",
    "# -------------------------------\n",
    "final_results = eval_mgr.get_summary()\n",
    "print(\"Final Sentiment Model Performance Summary (ordered by F1 score):\")\n",
    "display(final_results)\n"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [
    "73933381"
   ],
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python (tf-env)",
   "language": "python",
   "name": "tf-env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
